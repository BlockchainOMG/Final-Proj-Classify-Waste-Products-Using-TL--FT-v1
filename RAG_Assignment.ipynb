{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install langchain langchain-community langchain-ibm chromadb pypdf gradio python-dotenv ibm-watson-machine-learning -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_ibm import WatsonxEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_ibm import ChatWatsonx\n",
        "from langchain.prompts import PromptTemplate\n",
        "import gradio as gr\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "**Note**: You'll need to set up your watsonx.ai API credentials. Create a `.env` file with:\n",
        "- `WATSONX_APIKEY=your_api_key`\n",
        "- `WATSONX_PROJECT_ID=your_project_id`\n",
        "- Or set them as environment variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\", \"your_api_key_here\")\n",
        "WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\", \"your_project_id_here\")\n",
        "WATSONX_ENDPOINT = \"https://us-south.ml.cloud.ibm.com\"  # Adjust based on your region\n",
        "\n",
        "# Model configurations\n",
        "LLM_MODEL = \"mistralai/mixtral-8x7b-instruct-v01\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-minilm-l6-v2\"  # watsonx embedding model\n",
        "\n",
        "print(\"âœ“ Configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 1: Load Document Using LangChain for Different Sources (10 points)\n",
        "\n",
        "This task demonstrates loading documents from PDF sources using LangChain's PyPDFLoader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Load document using LangChain for different sources\n",
        "# Example: Loading PDF document\n",
        "\n",
        "def load_pdf_document(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Load a PDF document using LangChain's PyPDFLoader\n",
        "    \n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file (local or URL)\n",
        "    \n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    # Initialize the PDF loader\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    \n",
        "    # Load the document\n",
        "    documents = loader.load()\n",
        "    \n",
        "    print(f\"âœ“ Successfully loaded PDF: {pdf_path}\")\n",
        "    print(f\"âœ“ Number of pages: {len(documents)}\")\n",
        "    print(f\"âœ“ Total characters: {sum(len(doc.page_content) for doc in documents)}\")\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# Example usage - Uncomment and provide your PDF path\n",
        "# pdf_path = \"your_document.pdf\"  # Replace with your PDF path\n",
        "# documents = load_pdf_document(pdf_path)\n",
        "\n",
        "# Display first page content preview\n",
        "# if documents:\n",
        "#     print(\"\\nFirst page preview:\")\n",
        "#     print(documents[0].page_content[:500])\n",
        "\n",
        "print(\"âœ“ Task 1: PDF loader function created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'pdf_loader.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 2: Apply Text Splitting Techniques (10 points)\n",
        "\n",
        "This task demonstrates text splitting to enhance model responsiveness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Apply text splitting techniques to enhance model responsiveness\n",
        "\n",
        "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks using RecursiveCharacterTextSplitter\n",
        "    \n",
        "    Args:\n",
        "        documents: List of Document objects\n",
        "        chunk_size: Maximum size of each text chunk\n",
        "        chunk_overlap: Number of characters to overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of split Document objects\n",
        "    \"\"\"\n",
        "    # Initialize the text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    # Split the documents\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    \n",
        "    print(f\"âœ“ Original documents: {len(documents)}\")\n",
        "    print(f\"âœ“ Split into chunks: {len(split_docs)}\")\n",
        "    print(f\"âœ“ Average chunk size: {sum(len(doc.page_content) for doc in split_docs) // len(split_docs)} characters\")\n",
        "    print(f\"âœ“ Chunk size: {chunk_size}, Overlap: {chunk_overlap}\")\n",
        "    \n",
        "    return split_docs\n",
        "\n",
        "# Example usage\n",
        "# split_documents_list = split_documents(documents, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "print(\"âœ“ Task 2: Text splitter function created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'code_splitter.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 3: Embed Documents (10 points)\n",
        "\n",
        "This task demonstrates embedding documents using watsonx's embedding model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Embed documents using watsonx's embedding model\n",
        "\n",
        "def create_embeddings(api_key=None, project_id=None, endpoint=None):\n",
        "    \"\"\"\n",
        "    Create embeddings using watsonx embedding model\n",
        "    \n",
        "    Args:\n",
        "        api_key: Watsonx API key\n",
        "        project_id: Watsonx project ID\n",
        "        endpoint: Watsonx API endpoint\n",
        "    \n",
        "    Returns:\n",
        "        WatsonxEmbeddings object\n",
        "    \"\"\"\n",
        "    # Initialize watsonx embeddings\n",
        "    # Note: Adjust parameters based on your watsonx.ai setup\n",
        "    embeddings = WatsonxEmbeddings(\n",
        "        model_id=\"sentence-transformers/all-minilm-l6-v2\",  # or use watsonx embedding model\n",
        "        apikey=api_key or WATSONX_APIKEY,\n",
        "        project_id=project_id or WATSONX_PROJECT_ID,\n",
        "        url=endpoint or WATSONX_ENDPOINT\n",
        "    )\n",
        "    \n",
        "    print(\"âœ“ Embeddings model initialized successfully\")\n",
        "    print(f\"âœ“ Model: sentence-transformers/all-minilm-l6-v2\")\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Alternative: If watsonx embeddings are not available, you can use HuggingFace embeddings\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Initialize embeddings\n",
        "# embeddings = create_embeddings()\n",
        "\n",
        "print(\"âœ“ Task 3: Embedding function created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'embedding.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 4: Create and Configure Vector Database (10 points)\n",
        "\n",
        "This task demonstrates creating a Chroma vector database to store document embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Create and configure Chroma vector database to store embeddings\n",
        "\n",
        "def create_vector_database(split_documents, embeddings, persist_directory=\"./chroma_db\"):\n",
        "    \"\"\"\n",
        "    Create a Chroma vector database from split documents and embeddings\n",
        "    \n",
        "    Args:\n",
        "        split_documents: List of split Document objects\n",
        "        embeddings: Embeddings model\n",
        "        persist_directory: Directory to persist the vector database\n",
        "    \n",
        "    Returns:\n",
        "        Chroma vector store object\n",
        "    \"\"\"\n",
        "    # Create Chroma vector database\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=split_documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    \n",
        "    # Persist the vector database\n",
        "    vectorstore.persist()\n",
        "    \n",
        "    print(f\"âœ“ Vector database created successfully\")\n",
        "    print(f\"âœ“ Number of documents stored: {len(split_documents)}\")\n",
        "    print(f\"âœ“ Persist directory: {persist_directory}\")\n",
        "    \n",
        "    return vectorstore\n",
        "\n",
        "# Example usage\n",
        "# vectorstore = create_vector_database(split_documents_list, embeddings)\n",
        "\n",
        "print(\"âœ“ Task 4: Vector database creation function created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'vectordb.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 5: Develop a Retriever (10 points)\n",
        "\n",
        "This task demonstrates developing a retriever to fetch document segments based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5: Develop a retriever to fetch document segments based on queries\n",
        "\n",
        "def create_retriever(vectorstore, k=4, search_type=\"similarity\"):\n",
        "    \"\"\"\n",
        "    Create a retriever from the vector database\n",
        "    \n",
        "    Args:\n",
        "        vectorstore: Chroma vector store object\n",
        "        k: Number of documents to retrieve\n",
        "        search_type: Type of search (\"similarity\" or \"mmr\")\n",
        "    \n",
        "    Returns:\n",
        "        Retriever object\n",
        "    \"\"\"\n",
        "    # Create retriever from vectorstore\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=search_type,\n",
        "        search_kwargs={\"k\": k}\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ“ Retriever created successfully\")\n",
        "    print(f\"âœ“ Retrieval type: {search_type}\")\n",
        "    print(f\"âœ“ Number of documents to retrieve: {k}\")\n",
        "    \n",
        "    return retriever\n",
        "\n",
        "# Example usage\n",
        "# retriever = create_retriever(vectorstore, k=4)\n",
        "\n",
        "# Test retrieval with a sample query\n",
        "# query = \"What is the main topic of this paper?\"\n",
        "# retrieved_docs = retriever.get_relevant_documents(query)\n",
        "# print(f\"\\nRetrieved {len(retrieved_docs)} documents for query: '{query}'\")\n",
        "\n",
        "print(\"âœ“ Task 5: Retriever function created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'retriever.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Task 6: Construct QA Bot with Gradio Interface (10 points)\n",
        "\n",
        "This task demonstrates constructing a QA Bot that leverages LangChain and LLM to answer questions from loaded documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 6: Construct a QA Bot that leverages LangChain and LLM to answer questions\n",
        "\n",
        "def create_qa_chain(retriever, llm_model=LLM_MODEL, api_key=None, project_id=None, endpoint=None):\n",
        "    \"\"\"\n",
        "    Create a QA chain using RetrievalQA\n",
        "    \n",
        "    Args:\n",
        "        retriever: Retriever object\n",
        "        llm_model: LLM model identifier\n",
        "        api_key: Watsonx API key\n",
        "        project_id: Watsonx project ID\n",
        "        endpoint: Watsonx API endpoint\n",
        "    \n",
        "    Returns:\n",
        "        RetrievalQA chain object\n",
        "    \"\"\"\n",
        "    # Initialize the LLM\n",
        "    llm = ChatWatsonx(\n",
        "        model=llm_model,\n",
        "        apikey=api_key or WATSONX_APIKEY,\n",
        "        project_id=project_id or WATSONX_PROJECT_ID,\n",
        "        url=endpoint or WATSONX_ENDPOINT,\n",
        "        params={\n",
        "            \"decoding_method\": \"sample\",\n",
        "            \"max_new_tokens\": 1000,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Create prompt template\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    \n",
        "    Context: {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Answer:\"\"\"\n",
        "    \n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    \n",
        "    # Create QA chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ“ QA chain created successfully\")\n",
        "    print(f\"âœ“ LLM model: {llm_model}\")\n",
        "    \n",
        "    return qa_chain\n",
        "\n",
        "def qa_function(question, qa_chain):\n",
        "    \"\"\"\n",
        "    Answer a question using the QA chain\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        qa_chain: QA chain object\n",
        "    \n",
        "    Returns:\n",
        "        Answer string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "        answer = result[\"result\"]\n",
        "        sources = result.get(\"source_documents\", [])\n",
        "        \n",
        "        # Format the response\n",
        "        response = f\"**Answer:**\\n{answer}\\n\\n\"\n",
        "        \n",
        "        if sources:\n",
        "            response += f\"**Sources:** {len(sources)} document(s) retrieved\\n\"\n",
        "        \n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"âœ“ Task 6: QA chain functions created\")\n",
        "print(\"\\nðŸ“¸ Please take a screenshot of this code cell and save it as 'QA_bot.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gradio interface for the QA Bot\n",
        "\n",
        "def create_gradio_interface(qa_chain):\n",
        "    \"\"\"\n",
        "    Create a Gradio interface for the QA bot\n",
        "    \n",
        "    Args:\n",
        "        qa_chain: QA chain object\n",
        "    \n",
        "    Returns:\n",
        "        Gradio interface object\n",
        "    \"\"\"\n",
        "    def chat_interface(question, history):\n",
        "        \"\"\"Chat interface function for Gradio\"\"\"\n",
        "        if not question.strip():\n",
        "            return \"Please enter a question.\"\n",
        "        \n",
        "        try:\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "            answer = result[\"result\"]\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            return f\"Error processing your question: {str(e)}\"\n",
        "    \n",
        "    # Create Gradio interface\n",
        "    interface = gr.Interface(\n",
        "        fn=chat_interface,\n",
        "        inputs=gr.Textbox(\n",
        "            label=\"Question\",\n",
        "            placeholder=\"Ask a question about the document...\",\n",
        "            lines=3\n",
        "        ),\n",
        "        outputs=gr.Textbox(\n",
        "            label=\"Answer\",\n",
        "            lines=10\n",
        "        ),\n",
        "        title=\"Quest Analytics - AI RAG Assistant\",\n",
        "        description=\"Ask questions about the loaded documents. Upload a PDF and ask questions!\",\n",
        "        theme=\"default\"\n",
        "    )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "print(\"âœ“ Gradio interface function created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Implementation with File Upload\n",
        "\n",
        "This cell contains the complete implementation that allows uploading PDFs through the Gradio interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete implementation with file upload capability in Gradio\n",
        "# This allows users to upload PDFs directly through the interface\n",
        "\n",
        "def create_full_gradio_interface():\n",
        "    \"\"\"Create a complete Gradio interface with file upload\"\"\"\n",
        "    \n",
        "    # Store QA chain globally (in a real app, you'd use session state)\n",
        "    qa_chain_store = {\"chain\": None, \"vectorstore\": None}\n",
        "    \n",
        "    def upload_and_process(pdf_file):\n",
        "        \"\"\"Process uploaded PDF\"\"\"\n",
        "        if pdf_file is None:\n",
        "            return \"Please upload a PDF file.\", \"\"\n",
        "        \n",
        "        try:\n",
        "            # Step 1: Load PDF\n",
        "            documents = load_pdf_document(pdf_file.name)\n",
        "            \n",
        "            # Step 2: Split documents\n",
        "            split_docs = split_documents(documents)\n",
        "            \n",
        "            # Step 3: Create embeddings\n",
        "            embeddings = create_embeddings()\n",
        "            \n",
        "            # Step 4: Create vector database\n",
        "            vectorstore = create_vector_database(split_docs, embeddings, persist_directory=\"./temp_chroma_db\")\n",
        "            \n",
        "            # Step 5: Create retriever\n",
        "            retriever = create_retriever(vectorstore, k=4)\n",
        "            \n",
        "            # Step 6: Create QA chain\n",
        "            qa_chain = create_qa_chain(retriever)\n",
        "            \n",
        "            # Store for later use\n",
        "            qa_chain_store[\"chain\"] = qa_chain\n",
        "            qa_chain_store[\"vectorstore\"] = vectorstore\n",
        "            \n",
        "            return f\"âœ“ PDF processed successfully! {len(documents)} pages loaded, {len(split_docs)} chunks created.\", \"\"\n",
        "        except Exception as e:\n",
        "            return f\"Error processing PDF: {str(e)}\", \"\"\n",
        "    \n",
        "    def ask_question(question):\n",
        "        \"\"\"Ask a question about the uploaded document\"\"\"\n",
        "        if qa_chain_store[\"chain\"] is None:\n",
        "            return \"Please upload and process a PDF file first.\"\n",
        "        \n",
        "        if not question.strip():\n",
        "            return \"Please enter a question.\"\n",
        "        \n",
        "        try:\n",
        "            result = qa_chain_store[\"chain\"].invoke({\"query\": question})\n",
        "            return result[\"result\"]\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "    \n",
        "    with gr.Blocks(title=\"Quest Analytics - AI RAG Assistant\") as demo:\n",
        "        gr.Markdown(\"# Quest Analytics - AI RAG Assistant\")\n",
        "        gr.Markdown(\"Upload a PDF document and ask questions about it.\")\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                pdf_upload = gr.File(\n",
        "                    label=\"Upload PDF Document\",\n",
        "                    file_types=[\".pdf\"]\n",
        "                )\n",
        "                upload_btn = gr.Button(\"Process PDF\", variant=\"primary\")\n",
        "                upload_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "            \n",
        "            with gr.Column():\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"Ask a Question\",\n",
        "                    placeholder=\"What is this paper talking about?\",\n",
        "                    lines=3\n",
        "                )\n",
        "                ask_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
        "                answer_output = gr.Textbox(\n",
        "                    label=\"Answer\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "        \n",
        "        upload_btn.click(\n",
        "            fn=upload_and_process,\n",
        "            inputs=pdf_upload,\n",
        "            outputs=[upload_status, answer_output]\n",
        "        )\n",
        "        \n",
        "        ask_btn.click(\n",
        "            fn=ask_question,\n",
        "            inputs=question_input,\n",
        "            outputs=answer_output\n",
        "        )\n",
        "        \n",
        "        # Allow Enter key to submit question\n",
        "        question_input.submit(\n",
        "            fn=ask_question,\n",
        "            inputs=question_input,\n",
        "            outputs=answer_output\n",
        "        )\n",
        "    \n",
        "    return demo\n",
        "\n",
        "print(\"âœ“ Complete Gradio interface with file upload created\")\n",
        "print(\"\\nTo launch the interface, run:\")\n",
        "print(\"demo = create_full_gradio_interface()\")\n",
        "print(\"demo.launch(share=True)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Workflow - Run All Tasks Together\n",
        "\n",
        "Run this cell to execute the complete workflow with a PDF file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete workflow: Run all tasks together\n",
        "# Uncomment and modify the paths/credentials as needed\n",
        "\n",
        "# Step 1: Load PDF document\n",
        "# pdf_path = \"your_document.pdf\"  # Replace with your PDF path\n",
        "# documents = load_pdf_document(pdf_path)\n",
        "\n",
        "# Step 2: Split documents\n",
        "# split_documents_list = split_documents(documents, chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Step 3: Create embeddings\n",
        "# embeddings = create_embeddings()\n",
        "\n",
        "# Step 4: Create vector database\n",
        "# vectorstore = create_vector_database(split_documents_list, embeddings)\n",
        "\n",
        "# Step 5: Create retriever\n",
        "# retriever = create_retriever(vectorstore, k=4)\n",
        "\n",
        "# Step 6: Create QA chain\n",
        "# qa_chain = create_qa_chain(retriever)\n",
        "\n",
        "# Step 7: Test with a query\n",
        "# query = \"What is this paper talking about?\"\n",
        "# result = qa_chain.invoke({\"query\": query})\n",
        "# print(\"Question:\", query)\n",
        "# print(\"Answer:\", result[\"result\"])\n",
        "\n",
        "print(\"âœ“ Complete workflow code prepared\")\n",
        "print(\"\\nUncomment the code above and provide your PDF path to run the complete workflow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch the Gradio Interface\n",
        "\n",
        "Run this cell to launch the interactive Gradio interface for Task 6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface\n",
        "# Uncomment to run\n",
        "\n",
        "# demo = create_full_gradio_interface()\n",
        "# demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)\n",
        "\n",
        "print(\"âœ“ Ready to launch interface\")\n",
        "print(\"\\nðŸ“¸ For Task 6, take a screenshot of the Gradio interface showing:\")\n",
        "print(\"  1. A PDF file uploaded\")\n",
        "print(\"  2. The query 'What is this paper talking about?' entered\")\n",
        "print(\"  3. The answer displayed\")\n",
        "print(\"\\nSave the screenshot as 'QA_bot.png'\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
